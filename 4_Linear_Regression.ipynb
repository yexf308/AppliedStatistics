{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsoTyFVgY6aYltdqbqn2nD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/AppliedStatistics/blob/main/4_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31-3g5KvBupj",
        "outputId": "bc0c0cd6-45ad-4639-f23f-dd2a4fe01fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline \n",
        "import pandas as pd\n",
        "from scipy import linalg\n",
        "from itertools import combinations\n",
        "import scipy\n",
        "import time\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mm#1{\\boldsymbol{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "$\\def\\mr#1{\\mathrm{#1}}$\n",
        "$\\newenvironment{rmat}{\\left[\\begin{array}{rrrrrrrrrrrrr}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\brm{\\begin{rmat}}$\n",
        "$\\newcommand\\erm{\\end{rmat}}$\n",
        "$\\newenvironment{cmat}{\\left[\\begin{array}{ccccccccc}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\bcm{\\begin{cmat}}$\n",
        "$\\newcommand\\ecm{\\end{cmat}}$\n",
        "\n",
        "\n",
        "\n",
        "We have the training data $\\{\\m{x}^{(i)}, y^{(i)}\\}_{i=1}^N$, where $y$ is the output response and $x_j$ is the $j$-th input feature. \n",
        "\n",
        "We assume the relationship between $y$ and $\\m{x}$, (note $y$ is a random variable)\n",
        "\\begin{align}\n",
        "y=f(\\m{x})+\\epsilon\n",
        "\\end{align}\n",
        "$f$ is the fixed unknown function and $\\epsilon$ is a random error term and is independent of $\\m{x}$ and has mean zero. \n",
        "\n",
        "Say $\\hat f$ is our estimate for $f$, and we have the prediction $\\hat y = \\hat f(\\m{x})$. Assume $\\hat f$ and $\\m{x}$ are fixed, \n",
        "\\begin{align}\n",
        "\\mb{E}[(y-\\hat y)^2] &= \\mb{E}[(f(\\m{x})-\\hat f(\\m{x}))^2] \\\\\n",
        "&=(f(\\m{x})-\\hat f(\\m{x}))^2 +\\text{Var}(\\epsilon)\n",
        "\\end{align}\n",
        "The first term is the **reducible error** and the second term is **irreducible error**. \n",
        "# Multiple Linear Regression\n",
        "Assume the functional form or shape of $f$, here we assume $f$ is linear in $\\m{x}$, i.e.,\n",
        "\n",
        "\\begin{align}\n",
        "y = w_0 + w_1 x_1 + \\dots + w_dx_d + \\epsilon\n",
        "\\end{align}\n",
        "$ w_0 $ is the intercept. We can absorb the intercept term $w_0$.\n",
        "\n",
        "The **weights** vector $\\m{w}=[w_0, w_1, \\dots, w_d]^\\top\\in \\mb{R}^{d+1}$ and the **feature** vector $\\m{x}=[1, x_1, x_2, \\dots, x_d]\\in \\mb{R}^{d+1}$. Then \n",
        "\\begin{align}\n",
        "y = \\m{x}\\m{w} +\\epsilon\n",
        "\\end{align}\n",
        "\n",
        "$\\epsilon_i = y^{(i)}-\\mathbf{x}^{(i)}\\mathbf{w}$ is the residual error of $i$-th sample, and follows the Gaussian distrbution, $\\c{N}(0, \\sigma^2)$. \n",
        "\n",
        "## Least square estimation\n",
        "After the linear model is selected, we uses the traning data to fit the model. Here we introduces the **least square estimation**. \n",
        "\n",
        "- Define the residue sum of squares (RSS), $\\mm\\epsilon = [\\epsilon_1, \\dots, \\epsilon_N]$.\n",
        "\\begin{align}\n",
        "\\text{RSS}(\\m{w}) &= \\sum\\epsilon_i^2=\\|\\mm\\epsilon\\|_2^2  \\\\\n",
        "&=\\sum_{i=1}^N (y^{(i)}-\\mathbf{x}^{(i)}\\mathbf{w})^2 \\\\\n",
        "&=\\|\\mathbf{X}\\mathbf{w}-\\mathbf{y}\\|_2^2 = (\\mathbf{X}\\mathbf{w}-\\mathbf{y})^\\top(\\mathbf{X}\\mathbf{w}-\\mathbf{y})\n",
        "\\end{align}\n",
        "where $\\m{X}= \\bcm \\m{x}^{(1)} \\\\ \\m{x}^{(2)} \\\\ \\dots \\\\ \\m{x}^{(N)}\\ecm \\in \\mb{R}^{N\\times (d+1)}$ and $\\m{y}=\\bcm y^{(1)} \\\\ y^{(2)}\\\\ \\dots \\\\ y^{(N)}\\ecm$. Usually in regression, we have $N\\gg d+1$, then $\\text{rank}(\\mathbf{X})=d+1$ is of full rank very likely. \n",
        "\n",
        "- Mean squared error (MSE) is $\\text{MSE}(\\m{w}) = \\frac{1}{N}\\text{RSS}(\\m{w})$. Root Mean squared error (RMSE) is $\\text{RMSE}(\\m{w})=\\sqrt{\\text{MSE}(\\m{w})}$. \n",
        "\n",
        "- The gradient of RSS is \n",
        "$ \\nabla \\text{RSS}(\\mathbf{w})=2(\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w}-\\mathbf{X}^\\top\\mathbf{y})$ (why?)\n",
        "So to solve \n",
        "$$\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w}=\\mathbf{X}^\\top\\mathbf{y} $$\n",
        "The ordinary least square solution is $\\hat{\\mathbf{w}}= (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}$. \n",
        "\n",
        "- If $\\mathbf{X}$ is full rank, the hessian of RSS$(\\mathbf{w})$ is $\\mathbf{X}^\\top\\mathbf{X}$ is Positive definite matrix and $\\text{RSS}(\\m{w})$ is a strictly convex function! The least square solution is unique. \n",
        "\n",
        "\n",
        "- The quantity $\\mathbf{X}^\\dagger = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top$ is the **left pseudo inverse** of the non-square matrix $\\mathbf{X}$. \n",
        "\n",
        "- $\\hat{\\m{y}}$ is the predicted value $\\hat{\\m{y}}= \\m{X}\\hat{\\m{w}} = \\m{X}(\\m{X}^\\top\\m{X})^{-1}\\m{X}^\\top \\m{y}$.\n",
        "\n",
        "\n",
        "- $\\hat{\\m{w}}$ is MLE as well. Proof: \n",
        "   - The linear model can be written as $p(y|\\m{x},\\theta)= \\c{N}(y|\\m{x}\\m{w}, \\sigma^2)$, where $\\theta = (\\m{w},\\sigma^2)$. We assume $\\sigma$ is fixed. \n",
        "\n",
        "   - The NLL is \n",
        "   \\begin{align}\n",
        "   \\text{NLL}(\\m{w})= -\\sum_{i=1}^N \\log \\left[(2\\pi\\sigma^2)^{-1/2}\\exp\\left(-\\frac{(y^{(i)}-\\m{x}^{(i)}\\m{w})^2}{2\\sigma^2}\\right)\\right] = C_1 + C_2\\sum_{i=1}^N (y^{(i)}-\\m{x}^{(i)}\\m{w})^2\n",
        "   \\end{align}  \n",
        "\n",
        "  - Minmizing NLL is equivalent with minimizing RSS.  "
      ],
      "metadata": {
        "id": "NMJHliu-CpvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Geometric interpretation \n",
        "\n",
        "Minimizing RSS becomes the following geometry problem: find a vector $\\hat{\\m{y}}$ lies in the linear subspace spanned by $\\m{X}$ and is as close as possible to $\\m{y}$, i.e.,\n",
        "\\begin{align}\n",
        "\\text{argmin}_{\\hat{\\m{y}}\\in \\text{span}(\\m{X})} \\|\\m{y}-\\hat{\\m{y}}\\|_2\n",
        "\\end{align}\n",
        "\n",
        "### Orthognal projection\n",
        "Define the projection matrix $P=\\m{X}(\\m{X}^\\top\\m{X})^{-1}\\m{X}^\\top$ (verify the definition) and $\\text{span}(P)=\\text{span}(\\m{X})$, then $\\m{\\hat{y}}$ is projected value of $\\m{y}$ onto the $\\text{span}(\\m{X})$: $\\hat{\\m y}=P \\m{y}$. \n",
        "\n",
        "\n",
        "\n",
        "Note $\\m{X}^\\top (\\m{y}-\\hat{\\m{y}})=0$, so the residual vector to be orthogonal to the span$(\\mathbf{X})$.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/11.3.png?raw=true\" width=\"600\" />\n",
        "\n",
        "## Algorithm \n",
        "It is to solve the linear system, $Ax=b$, where $A = \\mathbf{X}^\\top\\mathbf{X}$ and $b= \\mathbf{X}^\\top\\mathbf{y}$.\n",
        "- Naive way: ```np.linalg.inv(X.T @ X) @ X.T @ y```\n",
        "\n",
        "- Pseudo-inverse(SVD): ```np.dot(np.linalg.pinv(X), y)```\n",
        "\n",
        "- lstsq(SVD-based solver): ```np.linalg.lstsq(X, y, rcond=None)[0] ```\n",
        "\n",
        "- QR decomposition:\n",
        "   ```\n",
        "   def qr_solve_over(X, y):\n",
        "    Q, R = np.linalg.qr(X)\n",
        "    Qy = np.dot(Q.T, y)\n",
        "    return scipy.linalg.solve_triangular(R, Qy)\n",
        "   ```\n",
        "\n",
        "\n",
        "Let's test with a tall near singular matrix \n"
      ],
      "metadata": {
        "id": "FYZzH-neeMiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def naive_solve(X, y):\n",
        "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def qr_solve(X, y):\n",
        "    Q, R = np.linalg.qr(X)\n",
        "    Qy = np.dot(Q.T, y)\n",
        "    return scipy.linalg.solve_triangular(R, Qy)\n",
        "\n",
        "\n",
        "\n",
        "def run_demo(m, n):\n",
        "    print(\"Solving linear system with {} constraints and {} unknowns\".format(m, n))\n",
        "    np.random.seed(0)\n",
        "    A = np.random.rand(m, n)\n",
        "    A[:,n-1]= np.sum(A[:,0:n-1],axis=1)\n",
        "    A = A +np.random.rand(m, n)*0.000001\n",
        "\n",
        "    b = np.random.rand(m, 1)\n",
        "\n",
        "    methods = list()\n",
        "    solns = list()\n",
        "\n",
        "    %time\n",
        "    methods.append(\"naive\")\n",
        "    solns.append(naive_solve(A, b))\n",
        "    \n",
        "    %time\n",
        "    methods.append(\"pinv\")\n",
        "    solns.append(np.dot(np.linalg.pinv(A), b))\n",
        "\n",
        "    %time\n",
        "    methods.append(\"lstsq\")\n",
        "    solns.append(np.linalg.lstsq(A, b, rcond=None)[0])\n",
        "    \n",
        "    %time\n",
        "    methods.append(\"qr\")\n",
        "    solns.append(qr_solve(A, b))\n",
        "\n",
        "    for (method, soln) in zip(methods, solns):\n",
        "        residual = b - np.dot(A, soln)\n",
        "        print(\n",
        "            \"method {}, norm {:0.5f}, residual {:0.5f}\".format(method, np.linalg.norm(soln), np.linalg.norm(residual))\n",
        "        )\n"
      ],
      "metadata": {
        "id": "22FLjmjMAssc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = 5000000\n",
        "n = 20  # Overdetermined\n",
        "run_demo(m, n)"
      ],
      "metadata": {
        "id": "BornmyewRvY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643aff42-6c83-4841-fe02-497ceab66817"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving linear system with 5000000 constraints and 20 unknowns\n",
            "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
            "Wall time: 12.2 µs\n",
            "CPU times: user 10 µs, sys: 2 µs, total: 12 µs\n",
            "Wall time: 12.2 µs\n",
            "CPU times: user 8 µs, sys: 1 µs, total: 9 µs\n",
            "Wall time: 10.5 µs\n",
            "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
            "Wall time: 8.11 µs\n",
            "method naive, norm 93722.33895, residual 654.75993\n",
            "method pinv, norm 112758.16887, residual 654.54737\n",
            "method lstsq, norm 112758.16887, residual 654.54737\n",
            "method qr, norm 112758.16887, residual 654.54737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Accuracy: pinv = lstsq = qr > naive\n",
        "- Time: lstsq > pinv > qr\n",
        "\n",
        "So for tall matrix ($N>d$), QR method is the best: quicker and accurate. \n",
        "\n",
        "In fact, for fat matrix ($N< d$), SVD method will be the best. "
      ],
      "metadata": {
        "id": "R8QPEuJ1Cxue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetical data\n"
      ],
      "metadata": {
        "id": "PGkP6FaOOpLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "N = 51\n",
        "x = np.linspace(0.0, 20, N)\n",
        "X0 = x.reshape(N, 1)\n",
        "X = np.c_[np.ones((N, 1)), X0]\n",
        "w = np.array([-1.5, 1 / 9.0])\n",
        "y = w[0] * x + w[1] * np.square(x)\n",
        "y = y + np.random.normal(0, 1, N) * 2\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.scatter(x, y)\n",
        "ax.set_xlabel('$x$', fontsize=13)\n",
        "ax.set_ylabel('$y$', fontsize=13)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jo7Dz2cxOo5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the most straightforward way to solve the least square. \n",
        "# w = np.linalg.lstsq(X, y, rcond=None)[0]\n",
        "w = qr_solve(X, y)\n",
        "\n",
        "\n",
        "W0, W1 = np.meshgrid(np.linspace(-15, 7, 300), np.linspace(-1.5, 2.5, 200))\n",
        "SS = np.array([sum((w0 * X[:, 0] + w1 * X[:, 1] - y) ** 2) for w0, w1 in zip(np.ravel(W0), np.ravel(W1))])\n",
        "SS = SS.reshape(W0.shape)\n",
        "\n",
        "fig = go.Figure(data=[go.Surface(z=SS, x=W0, y=W1)])\n",
        "fig.update_traces(contours_z=dict(show=True, usecolormap=True,\n",
        "                                  highlightcolor=\"limegreen\", project_z=True))\n"
      ],
      "metadata": {
        "id": "qC-vH1AwQ_o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "CS = plt.contour(W0, W1, SS, levels=np.linspace(0, 2000, 10), cmap=\"jet\")\n",
        "plt.plot(w[0], w[1], \"x\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ogjISzEeWoKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.scatter(x, y)\n",
        "ax.plot(x, w[0]+w[1]*x)\n",
        "ax.set_xlabel('$x$', fontsize=13)\n",
        "ax.set_ylabel('$y$', fontsize=13)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ha8AVJjwSTej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nonlinear Relationships\n",
        " In general a straight line will not provide a good fit to most datasets. \n",
        "If $x\\in \\mb{R}$, which is 1D, we can apply a nonlinear transformation to the input features, by replacing $x$ with $\\phi(x)$ to get\n",
        "$$p(y|x, \\theta) = \\mathcal{N}(y|\\phi(x)\\mathbf{w}, \\sigma^2) $$\n",
        "As long as $\\phi$ is fixed, the model remains linear in the parameter. One possible choice is to use a polynomial expansion of degree $d$. Now $d$ is the parameter. \n",
        "\\begin{align}\n",
        "\\phi(x) = [1, x, x^2, \\dots, x^d ]\n",
        "\\end{align}\n",
        "But now the question is how to choose the parameter $d$?\n",
        "- small $d$: Underfitting, the predicted value deviate from observed value a lot. \n",
        "- Large $d$: Overfitting, it cannot generalize to future dataset. \n",
        "\n",
        "If $\\m{x}\\in \\mb{R}^p$, we can generate $\\m{u}_1, \\dots, \\m{u}_d \\in \\mb{R}^p$, then \n",
        "\\begin{align}\n",
        "\\phi(\\m{x}) = [h_1(\\m{x}), \\dots, h_d(\\m{x})].\n",
        "\\end{align}\n",
        "where $h_j(\\m{x})= \\frac{1}{1+\\exp(\\m{x}\\m{u}_j^\\top)}$ or $(\\m{x}\\m{u}_j^\\top)^2$ or $\\cos(\\m{x}\\m{u}_j^\\top)$.  \n",
        "\n",
        "### Overfitting and Holding out\n",
        "Fit our regression methods on training dataset $\\{\\m{x}^{(i)}, y^{(i)}\\}_{i=1}^N$ and compute predicted value $\\hat f(\\m{x}^{(i)})$. If these predicted values are approximately equal to $y^{(i)}$, then RSS/MSE/NLL will be small. But we are not interested in whether $\\hat f(\\m{x}^{(i)}) \\approx y^{(i)}$. Instead we want to know whether $\\hat f(\\m{x}^{(0)})$ is equal to $y^{(0)}$, where $\\{\\hat f(\\m{x}^{(0)}), y^{(0)}\\}$ is previously unseen validation observation not used to train the model. \n",
        "\n",
        "**Goal**: to choose the method gives the lowest test MSE, rather than the lowest training MSE. If we had a large number of test data, we want to minmize $\\mb{E}[(y^{(0)}-\\hat f(\\m{x}^{(0)}))^2]$\n",
        "\n",
        "**Method:** Holding out! Denote the ERM as $R_d(\\mm\\theta, \\c{D})=\\frac{1}{|\\c{D}|}\\sum_{\\c{D}}\\ell(\\m{y}, f_d(\\m{x};\\mm\\theta))$. \n",
        "\n",
        "- Partition the data into two disjoint sets, the training set $\\c{D}_{\\text{train}}$ and the testing(validation) set $\\c{D}_{\\text{test}}$, $\\c{D}=\\c{D}_{\\text{test}} \\cup \\c{D}_{\\text{train}}$. Usually, 80\\% of data for the training and 20\\% of data for the testing. \n",
        "\n",
        "\n",
        "- Fit the model on $\\c{D}_{\\text{train}}$ and evaluate its performance on $\\c{D}_{\\text{test}}$.\n",
        "\n",
        " - Compute the parameter estimate \n",
        "  \\begin{align}\n",
        "  \\hat{\\mm\\theta}_d(\\c{D}_{\\text{train}}) = \\arg\\min_{\\mm\\theta} R_d(\\mm\\theta, \\c{D}_{\\text{train}}) \n",
        "  \\end{align}\n",
        "\n",
        " - Compute the testing error\n",
        " \\begin{align}\n",
        " R_d^{\\text{val}} = R_d(\\hat{\\mm\\theta}_d(\\c{D}_{\\text{train}}), \\c{D}_{\\text{test}})\n",
        " \\end{align}\n",
        "\n",
        "\n",
        "- Choose the value of parameter $d$ that results in the best validation performance.  This requires fitting the model once for each value of $d$ (sometimes one can perform this more efficiently). \n",
        "\\begin{align}\n",
        "d^* = \\arg\\min_{d} R_{d}^{\\text{val}}\n",
        "\\end{align}\n",
        "\n",
        "- (optional) After picking the optimal parameter, we can refit the model to the entire dataset $\\c{D}$, \n",
        "\\begin{align}\n",
        "\\hat{\\mm\\theta}_{d^*}(\\c{D}) = \\arg\\min_{\\mm\\theta} R_{d^*}(\\mm\\theta, \\c{D}) \n",
        "\\end{align}\n",
        "\n",
        "There are many other methods, such as cross-validation. We will talk about it later. \n",
        "\n"
      ],
      "metadata": {
        "id": "NgLG0SO5M5G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for simplicity, use train_test_split function in sklearn. You can write your own \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "d_span = np.arange(1,20)\n",
        "\n",
        "# hold out validation\n",
        "N_train        = len(x_train)\n",
        "X0             = x_train.reshape(N_train, 1)\n",
        "X_train        = np.ones((N_train, 1))\n",
        "\n",
        "N_test        = len(x_test)\n",
        "X1            = x_test.reshape(N_test, 1)\n",
        "X_test        = np.ones((N_test, 1))\n",
        "\n",
        "w_store       = []\n",
        "RSS_train     = []\n",
        "RSS_test      = []\n",
        "\n",
        "for d in d_span:\n",
        "  X_train = np.hstack([X_train, X0**d])\n",
        "  X_test  = np.hstack([X_test,  X1**d])\n",
        "  w       = qr_solve(X_train, y_train)\n",
        "  w_store.append(w)\n",
        "  y_train_hat = X_train.dot(w)\n",
        "  y_test_hat = X_test.dot(w)\n",
        "  RSS_train.append(norm(y_train-y_train_hat))\n",
        "  RSS_test.append(norm(y_test-y_test_hat))\n",
        "\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.plot(d_span, RSS_train)\n",
        "ax.plot(d_span, RSS_test)\n",
        "ax.set_xlabel('$d$', fontsize=13)\n",
        "ax.set_ylabel('$RSS$', fontsize=13)\n",
        "ax.legend(['Train', 'Test'])\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "BQXtJ166nByA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_plot_span = [1,2,19]\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.scatter(x,y)\n",
        "\n",
        "N        = len(x)\n",
        "X0       = x.reshape(N, 1)\n",
        "\n",
        "for d in d_plot_span:\n",
        "  X        = np.ones((N, 1))\n",
        "  for i in range(1,d+1):\n",
        "    X   = np.hstack([X, X0**i])\n",
        "\n",
        "  y_hat = X.dot(w_store[d-1])\n",
        "  ax.plot(x, y_hat)\n",
        "\n",
        "ax.set_xlabel('$x$', fontsize=13)\n",
        "ax.set_ylabel('$y$', fontsize=13)\n",
        "ax.legend(['1st','2nd','19th','data'])\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "urAYjdnnzGEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shows very clearly 2nd order polynormial fits the best. \n",
        "\n",
        "\n",
        "### Additional tests!"
      ],
      "metadata": {
        "id": "flVL_5-j3r_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
        "fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
        "\n",
        "N = 70\n",
        "x = np.linspace(0.0, 20, N)\n",
        "X0 = x.reshape(N, 1)\n",
        "X = np.c_[np.ones((N, 1)), X0]\n",
        "w = np.array([-1.5, 1 / 9.0])\n",
        "y = w[0] * x + w[1] * np.square(x)\n",
        "y = y + np.random.normal(0, 1, N) * 2\n",
        "\n",
        "\n",
        "for i in range(20):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "  d_span = np.arange(1,15)\n",
        "\n",
        "  # hold out validation\n",
        "  N_train        = len(x_train)\n",
        "  X0             = x_train.reshape(N_train, 1)\n",
        "  X_train        = np.ones((N_train, 1))\n",
        "\n",
        "  N_test        = len(x_test)\n",
        "  X1            = x_test.reshape(N_test, 1)\n",
        "  X_test        = np.ones((N_test, 1))\n",
        "\n",
        "  w_store       = []\n",
        "  RSS_train     = []\n",
        "  RSS_test      = []\n",
        "\n",
        "  for d in d_span:\n",
        "    X_train = np.hstack([X_train, X0**d])\n",
        "    X_test  = np.hstack([X_test,  X1**d])\n",
        "    w       = qr_solve(X_train, y_train)\n",
        "    w_store.append(w)\n",
        "    y_train_hat = X_train.dot(w)\n",
        "    y_test_hat = X_test.dot(w)\n",
        "    RSS_train.append(norm(y_train-y_train_hat))\n",
        "    RSS_test.append(norm(y_test-y_test_hat))\n",
        "  \n",
        "  ax1.plot(d_span, RSS_train)\n",
        "  ax2.plot(d_span, RSS_test)\n",
        "\n",
        "\n",
        "ax1.set_xlabel('$d$', fontsize=13)\n",
        "ax1.set_ylabel('RSS for train', fontsize=13)\n",
        "ax2.set_xlabel('$d$', fontsize=13)\n",
        "ax2.set_ylabel('RSS for test', fontsize=13)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NW0VXNY1wyKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- small $d$: high bias, low variance\n",
        "\n",
        "- large $d$, low bias, high variance"
      ],
      "metadata": {
        "id": "wR8sMsklEn9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All ten\n",
        "curves indicate that the model with a quadratic term has a dramatically\n",
        "smaller testing set RSS than the model with only a linear term. Furthermore,\n",
        "all ten curves indicate that there is not much benefit in including\n",
        "cubic or higher-order polynomial terms in the model. But it is worth noting\n",
        "that each of the ten curves results in a different test RSS estimate for each\n",
        "of the ten regression models considered. And there is no consensus among\n",
        "the curves as to which model results in the smallest testing set RSS.\n",
        "Based on the variability among these curves, all that we can conclude with\n",
        "any confidence is that the linear fit is not adequate for this data.\n",
        "\n",
        "- the validation estimate\n",
        "of the test error rate can be highly variable, depending on precisely\n",
        "which observations are included in the training set and which\n",
        "observations are included in the testing set.\n",
        "\n",
        "- In the validation approach, only a subset of the observations—those\n",
        "that are included in the training set rather than in the testing\n",
        "set are used to fit the model. Since statistical methods tend to perform\n",
        "worse when trained on fewer observations, this suggests that the\n",
        "testing set error rate may tend to overestimate the test error rate\n",
        "for the model fit on the entire data set."
      ],
      "metadata": {
        "id": "vUbdLkX4xg0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robust linear regression\n",
        "\n",
        "Least squares tends to produce a linear line that focused on trying to minimize these large errors { often due to outliers in dataset.)\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/least_abs_dev.png?raw=true\" width=\"400\" />\n",
        "\n",
        "The objective function is defined by the sum of absolute errors\n",
        "$$\\ell(\\mathbf{w})= \\|\\mm{\\epsilon}\\|_1=\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_1$$ \n",
        "\n",
        "Here are some optimization methods to minimize objective function: \n",
        "\n",
        "\n",
        "- coordinate gradient descent algorithm: $|\\varepsilon_i|$ is non-differentiable at $\\varepsilon_i = 0$, but sub-derivatives exist. \n",
        "\n",
        "- Huber loss surrogate: a smooth surrogate of $|\\varepsilon_i|$;  can be minimized via standard gradient descent. (smooth relaxation)\n",
        "\n",
        "\\begin{equation}\n",
        "|\\varepsilon_i| \\approx L_\\delta(\\varepsilon_i) = \n",
        "\\begin{cases}\n",
        "\\frac{\\varepsilon_i^2}{2\\delta} & \\mbox{if} \\; |\\varepsilon_i| \\leq \\delta \\\\\n",
        " |\\varepsilon_i| - \\frac{\\delta}{2} & \\mbox{if} \\; |\\varepsilon_i| > \\delta\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Omx0Fe2oAJ8a"
      }
    }
  ]
}