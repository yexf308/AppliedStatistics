{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOmUavIZbx3qOzy1HxJ+m4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/AppliedStatistics/blob/main/6_Kernalized_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRuAvL1z7yk3"
      },
      "outputs": [],
      "source": [
        "%pylab inline \n",
        "import numpy.linalg as LA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mm#1{\\boldsymbol{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "$\\def\\mr#1{\\mathrm{#1}}$\n",
        "$\\newenvironment{rmat}{\\left[\\begin{array}{rrrrrrrrrrrrr}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\brm{\\begin{rmat}}$\n",
        "$\\newcommand\\erm{\\end{rmat}}$\n",
        "$\\newenvironment{cmat}{\\left[\\begin{array}{ccccccccc}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\bcm{\\begin{cmat}}$\n",
        "$\\newcommand\\ecm{\\end{cmat}}$\n"
      ],
      "metadata": {
        "id": "PPTE7gxE_n72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernalized Regression\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/map_to_polar.png?raw=true\" width=\"800\" />\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/map_to_high_d.png?raw=true\" width=\"800\" />\n",
        "\n",
        "Many times data has the nonlinear features. In general, it is easier to classify/regress in high dimensional feature space. However, it is hard to know which feature map will work for given data. So the rule of thumb is to use lots of high-dimensional features and hope our algorithm will automatically pick the right set of features. \n",
        "\n",
        "To be more precise, **feature mapping $\\mm\\phi$**: $\\mb{R}^d \\rightarrow \\mb{R}^p$. It maps the original data into a rich and high-dimensional feature space. ($p\\gg d$). \n",
        "\n",
        "Example: \n",
        "\n",
        "- $d=1$, we can use the following polynomial basis. \n",
        "\\begin{align}\n",
        "\\mm\\phi(x) = \\bcm \\phi_1(x) \\\\ \\vdots \\\\ \\phi_k(x)\\ecm =\\bcm x \\\\ \\vdots \\\\ x^k  \\ecm\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "- $d>1$, we can (randomly) generate vectors $\\{\\m{u}_j\\}_{j=1}^p \\subset \\mb{R}^d$ and define possible features are listed here\n",
        "\\begin{align}\n",
        "&\\phi_j(\\m{x}) = \\cos(\\m{u}_j^{\\top} \\m{x}) \\\\\n",
        "&\\phi_j(\\m{x}) = (\\m{u}_j^{\\top} \\m{x})^2 \\\\ \n",
        "&\\phi_j(\\m{x}) = \\frac{1}{1+\\exp(\\m{u}_j^{\\top} \\m{x})}\n",
        "\\end{align} \n",
        "\n",
        "\n",
        "\n",
        "So feature space cab get really large really quickly. \n",
        "\n",
        "**Question:** \n",
        "\n",
        "- How many coefficients/parameters are there for degree-$k$ polynomials for $\\m{x}=[x_1, x_2, \\dots, x_d]\\in \\mb{R}^d$?\n",
        "\n",
        "- When $d< N\\ll p$, do we need the huge memory to store these features $\\{\\mm\\phi(\\m{x}^{(i)})\\}_{i=1}^N$ and large run time?? How do we deal with high-dimensional lifts of the data? \n",
        "\n",
        "## **A fundamental trick in ML: use kernels.**\n",
        "\n",
        "**Definition**: A function $K: \\mb{R}^d \\times \\mb{R}^d \\rightarrow \\mb{R}$ is a **kernel** for a map $\\mm\\phi$ if $K(\\m{x},\\m{x}')=\\mm\\phi(\\m{x})\\cdot \\phi(\\m{x}')$ for all $\\m{x}, \\m{x}'$.\n",
        "\n",
        "Note $K(\\m{x},\\m{x}')\\in \\mb{R}^N\\times \\mb{R}^N$ matrix. \n",
        "\n",
        "**Main idea:** We can represent our **training algorithm** and **prediction rule** as functions of $K(\\m{x},\\m{x}')$. This we can avoid explicitly computing and storing the high-dimensional features $\\{\\mm\\phi(\\m{x}^{(i)})\\}_{i=1}^N$. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "asPA_PPO_ii8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge Regression as Kernels\n",
        "Note the objective function is \n",
        "\\begin{align}\n",
        "\\hat{\\m{w}}=\\arg\\min_{\\m{w}\\in\\mb{R}^d}\\|\\m{y}-\\m{X}\\m{w}\\|_2^2 +\\lambda \\|\\m{w}\\|_2^2\n",
        "\\end{align}\n",
        "\n",
        "Just for exercise, we represent prediction with $\\hat{\\m{w}}$ using liear kernel defined as $K(\\m{x},\\m{x}')=\\m{x} (\\m{x}')^\\top$. So $K = \\m{X}\\m{X}^\\top$. \n",
        "\n",
        "- Assuming $N< d$, i.e., number of features are more than number of data points. \n",
        "\n",
        "- Training: we have the analytical solutions,\n",
        "\\begin{align}\n",
        "\\hat{\\m{w}} &= (\\m{X}^\\top \\m{X}+\\lambda \\m{I}_{d\\times d})^{-1}\\m{X}^\\top \\m{y} \\\\\n",
        "&= \\m{X}^\\top (\\m{X}\\m{X}^\\top+\\lambda \\m{I}_{N\\times N})^{-1} \\m{y} \n",
        "\\end{align}\n",
        "Need proof.\n",
        "- **Lemma:** $(P^{-1}+B^\\top R^{-1}B)^{-1} B^\\top R^{-1}= PB^\\top(BPB^\\top +R)^{-1}$. $B$ may not be a square matrix. \n",
        " \n",
        " Proof: It is equivalent with \n",
        " \\begin{align}\n",
        " (P^{-1}+B^\\top R^{-1}B)PB^\\top &= B^\\top + B^\\top R^{-1} B P B^{\\top} \\\\\n",
        " & = B^\\top R^{-1}(BPB^\\top +R)\n",
        " \\end{align} \n",
        " If $B\\in \\mb{R}^{m\\times n}$, $(BPB^\\top +R)\\in \\mb{R}^{m\\times m}$ and $(P^{-1}+B^\\top R^{-1}B)\\in \\mb{R}^{n\\times n}$. \n",
        "\n",
        "\n",
        "- $P=\\m{I}_{d\\times d}, B = \\m{X}, R= \\lambda \\m{I}_{N\\times N}$, then\n",
        "\\begin{align}\n",
        "&\\text{RHS} = (\\m{I}_{d\\times d}+\\m{X}^\\top(\\lambda^{-1}\\m{I}_{N\\times N})\\m{X})^{-1}\\m{X}^\\top (\\lambda^{-1}\\m{I}_{N\\times N}) = (\\m{X}^\\top \\m{X}+\\lambda \\m{I}_{d\\times d})^{-1}\\m{X}^\\top \\\\\n",
        "&\\text{LHS} = \\m{I}_{d\\times d} \\m{X}^\\top (\\m{X}\\m{X}^\\top + \\lambda \\m{I}_{N\\times N})^{-1}\n",
        "\\end{align}\n",
        "\n",
        "- Prediction: for a new data $\\m{x}\\in \\mb{R}^d$, the predicted value $y=\\m{x}\\hat{\\m{w}}= \\boxed{\\m{x}\\m{X}^\\top} ( \\boxed{\\m{X}\\m{X}^\\top}+\\lambda \\m{I}_{N\\times N})^{-1} \\m{y} $.\n",
        "\n",
        "- To make prediction on any future data points, all we need to know is \n",
        "\\begin{align}\n",
        "&\\c{K}(\\m{x}) \\triangleq \\bcm K(\\m{x}, \\m{x}^{(1)}) & K(\\m{x}, \\m{x}^{(2)}) & \\dots & K(\\m{x}, \\m{x}^{(N)})\\ecm =\\m{x}\\m{X}\\\\\n",
        "&\\m{K}\\triangleq \\bcm K(\\m{x}^{(1)}, \\m{x}^{(1)}) &K(\\m{x}^{(1)}, \\m{x}^{(2)}) & \\dots & K(\\m{x}^{(1)}, \\m{x}^{(N)}) \\\\ \n",
        "K(\\m{x}^{(2)}, \\m{x}^{(1)}) &K(\\m{x}^{(2)}, \\m{x}^{(2)}) & \\dots & K(\\m{x}^{(2)}, \\m{x}^{(N)}) \\\\\n",
        "\\dots & \\dots &\\dots &\\dots \\\\\n",
        "K(\\m{x}^{(N)}, \\m{x}^{(1)}) &K(\\m{x}^{(N)}, \\m{x}^{(2)}) & \\dots & K(\\m{x}^{(N)}, \\m{x}^{(N)})\n",
        "\\ecm=\\m{X}\\m{X}^\\top  \\in \\mb{R}^{N\\times N}\n",
        "\\end{align}\n",
        "Even if we run ridge linear regression on feature map $\\mm\\phi(\\m{x})$ we only need to\n",
        "access the features via kernel $K(\\m{x}^{(i)}, \\m{x}^{(j)})$ and $K(\\m{x}, \\m{x}^{(i)})$ and not features $\\mm\\phi(\\m{x})$. \n",
        "\n",
        "### Kernel of Polynomial features\n",
        "Consider polynomial features of degree exactly $k$, for example, $d=2$ and $k=2$, the feature map is \n",
        "\\begin{align}\n",
        "\\mm\\phi(\\m{x})= \\bcm x_1^2 & x_2^2 & \\sqrt{2}x_1x_2\\ecm\n",
        "\\end{align}\n",
        "Then $K(\\m{x}, \\m{x}')=\\mm\\phi(\\m{x})\\mm\\phi(\\m{x}')^\\top=(x_1x_1'+x_2x_2')^2 = (\\m{x}^\\top \\m{x})^2$\n",
        "\n",
        "For general $d$, \n",
        "\\begin{align}\n",
        "\\mm\\phi(\\m{x})= \\bcm x_1^2 & \\dots & x_d^2 &\\sqrt{2}x_1x_2 &\\dots & \\sqrt{2}x_{d-1}x_d\\ecm \\in \\mb{R}^{d(d+1)/2}\n",
        "\\end{align}\n",
        "Then $K(\\m{x}, \\m{x}') = (\\m{x}^\\top \\m{x})^2$ still the same. \n",
        "\n",
        "If we want to consider all polynomial features of degree up to degree $k$, for example $k=2, d=2$,\n",
        "\\begin{align}\n",
        "\\mm\\phi(\\m{x})= \\bcm 1& \\sqrt{2}x_1& \\sqrt{2}x_2& x_1^2 & x_2^2 &\\sqrt{2}x_1x_2\\ecm\n",
        "\\end{align} \n",
        "Then the kernel $K(\\m{x}, \\m{x}') = (1+\\m{x}\\m{x}')^2 $.\n"
      ],
      "metadata": {
        "id": "UUOznKzyVpBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complexity analysis \n",
        "- Note that for a data point $\\m{x}$, explicitly computing the feature $\\mm{\\phi}(\\m{x})$ takes $p=d^k$ time/memory. However $\\c{K}(\\m{x})$ will cost $Nd$ in time and $N$ in space.\n",
        "  - The features are implicit and accessed only via kernels, making it efficient. \n",
        "\n",
        "- In terms of training, what are the difference of time and space complexity?   \n",
        "  - The kernel trick needs $N^2d$ to calculate $K$ and $N^3$ to solve matrix-vector equation. So time complexity is $O(N^3 + N^2d)$. The space complexity is $O(N^2)$.\n",
        "\n",
        "  - The traditional methods needs time complexity $O(p^3+ p^2N)$ and the space complexity is $O(Np)$. \n",
        "\n"
      ],
      "metadata": {
        "id": "BpuRCwjby1RG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The kernel Trick\n",
        "Given dataset $\\c{D}=\\{\\m{x}^{(i)},\\m{y}^{(i)}\\}_{i=1}^N$,  pick a kernel $K: \\mb{R}^d\\times \\mb{R}^d \\rightarrow \\mb{R}$. \n",
        "\n",
        "- For a choice of a loss, use a linear predictor of the form \n",
        "$$\\hat{\\m{w}}= \\sum_{i=1}^N \\alpha_i (\\mm{\\phi}(\\m{x}^{(i)}))^\\top $$\n",
        "for some $\\mm\\alpha = [\\alpha_1 , \\dots, \\alpha_N]^\\top\\in \\mb{R}^N$ to be learned. \n",
        "\n",
        "\n",
        "\n",
        "- Design an algorithm that finds $\\mm\\alpha$ while accessing the data only via the kernel matrix $\\m{K}$ (i.e., only using $\\mm\\phi(\\m{x}^{(i)})\\mm\\phi(\\m{x}^{(j)})^\\top$). \n",
        "\n",
        "\n",
        "- Prediction for the new data $\\m{x}$ is  $y = \\mm\\phi(\\m{x})\\hat{\\m{w}} = \\sum_{i=1}^N \\alpha_i \\mm\\phi(\\m{x})(\\mm{\\phi}(\\m{x}^{(i)}))^\\top =  \\c{K}(\\m{x})\\mm\\alpha  $.  \n",
        "\n",
        "\n",
        "### Kernalized ridge regression\n",
        "\n",
        "Note the objective function is \n",
        "\\begin{align}\n",
        "\\hat{\\m{w}}=\\arg\\min_{\\m{w}\\in\\mb{R}^d}\\sum_{i=1}^N \\|y^{(i)}-\\mm\\phi(\\m{x}^{(i)})\\m{w}\\|_2^2 +\\lambda \\|\\m{w}\\|_2^2\n",
        "\\end{align}\n",
        "\n",
        "- There exists a linear predictor, $\\hat{\\m{w}}= \\sum_{i=1}^N \\alpha_i (\\mm{\\phi}(\\m{x}^{(i)}))^\\top $ for some $\\mm\\alpha = [\\alpha_1 , \\dots, \\alpha_N]^\\top\\in \\mb{R}^N$ to be learned. \n",
        "\n",
        "- The optimization becomes \n",
        "\\begin{align}\n",
        "\\hat{\\mm\\alpha} &= \\arg\\min_{\\mm\\alpha \\in\\mb{R}^N} \\sum_{i=1}^N \\|y^{(i)}-\\sum_{j=1}^N \\alpha_j \\mm{\\phi}(\\m{x}^{(i)}) (\\mm{\\phi}(\\m{x}^{(j)}))^\\top \\|_2^2 + \\lambda \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i\\alpha_j  \\mm{\\phi}(\\m{x}^{(i)}) (\\mm{\\phi}(\\m{x}^{(j)}))^\\top \\\\\n",
        "&=\\arg\\min_{\\mm\\alpha \\in\\mb{R}^N} \\|\\m{y} - \\m{K}\\mm\\alpha \\|_2^2 +\\lambda \\mm\\alpha^\\top \\m{K}\\mm\\alpha\n",
        "\\end{align}\n",
        "\n",
        "- Then $\\hat{\\mm\\alpha}= (\\m{K}+\\lambda \\m{I}_{N\\times N})^{-1}\\m{y}$.\n",
        "\n",
        "\n",
        "### The most general case \n",
        "- Once we have chosen to use a feature map $\\mm\\phi(\\m{x})\\in \\mb{R}^p$, and what we want to solve is \n",
        "\\begin{align}\n",
        "\\hat{\\m{w}}= \\arg\\min_{\\m{w}}\\sum_{i=1}^N \\ell(\\mm\\phi(\\m{x}^{(i)})\\m{w}, y^{(i)})\n",
        "\\end{align}\n",
        "for some convex loss $\\ell$.\n",
        "\n",
        "- Gradient descent update (from initialization $\\m{w}^{(0)}=0$ ) that find the optimal solution is\n",
        "\\begin{align}\n",
        "\\m{w}^{(t+1)}=\\m{w}^{(t)}-\\eta \\sum_{i=1}^N\\ell'(\\mm\\phi(\\m{x}^{(i)})\\m{w}, y^{(i)})(\\mm\\phi(\\m{x}^{(i)}))^\\top\n",
        "\\end{align} \n",
        "\n",
        "- One crucial observation is that all $\\m{w}^{(t)}$  (including the optimal solution $\\m{w}^{(\\infty)}$) lie on\n",
        "the subspace spanned by $\\{\\mm\\phi(\\m{x}^{(i)}\\}_{i=1}^N$ which is an $N$-dimensional subspace\n",
        "in $\\mb{R}^p$. \n",
        "\n",
        "- Hence, it is sufficient to look for a solution that is represented as  $\\hat{\\m{w}}= \\sum_{i=1}^N\\alpha_i \\mm\\phi(\\m{x}^{(i)})$ to find the optimal solution. \n",
        "\n",
        "- Kernel trick finds the optimal solution efficiently, by searching over the model that\n",
        "can be represented as $\\hat{\\m{w}}= \\sum_{i=1}^N\\alpha_i \\mm\\phi(\\m{x}^{(i)})$. "
      ],
      "metadata": {
        "id": "sVIwgOa31Yux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Gaussian kernels \n",
        "Gaussian (squared exponential) kernels. (a.k.a RBF kernel for Radial Basis Function). \n",
        "$$K(\\m{x}, \\m{x}')= \\exp\\left(-\\frac{\\|\\m{x}-\\m{x}'\\|_2^2}{2\\sigma^2}\\right) $$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/yexf308/AppliedStatistics/blob/main/image/rbf1.png?raw=true\" width=\"800\" />\n",
        "\n",
        "Predictor $f(\\m{x})= \\sum_{i=1}^N \\alpha_i K(\\m{x}, \\m{x}^{(i)})$ is taking weighted sum of $N$ kernel functions\n",
        "centered at each sample points.\n",
        "\n",
        "<img src=\"https://github.com/yexf308/AppliedStatistics/blob/main/image/rbf2.png?raw=true\" width=\"800\" />\n",
        "\n"
      ],
      "metadata": {
        "id": "Sl4xYj86yiEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When $N$ is very large: use random features.\n",
        "**Features vs. RBF kernel vs. random features**\n",
        "\n",
        "If N is very large, allocating an $N\\times N$ matrix is tough.\n",
        "Instead, consider generating random feature maps of the form:\n",
        "\\begin{align}\n",
        "\\mm\\phi(\\m{x}) = \\bcm \\sqrt{2}\\cos(\\m{x}\\m{w}_1 + b_1) &\\dots&   \\sqrt{2}\\cos(\\m{x}\\m{w}_{\\tilde p} + b_{\\tilde p})\\ecm \n",
        "\\end{align}\n",
        "where $\\m{w}_i\\sim \\c{N}(0, 2\\gamma \\m{I}_d)$ and $b_i\\sim \\text{Uniform}(0, \\pi)$ with $\\tilde p\\ll N\\ll p$. \n",
        "\n",
        "One can show that (Rahimi, Recht NIPS 2007)\n",
        "\\begin{align}\n",
        "\\mb{E}_{\\m{w},b}\\left[\\frac{1}{p}\\mm\\phi(\\m{x})(\\mm\\phi(\\m{x}'))^\\top\\right] = \\exp(-\\gamma \\|\\m{x}-\\m{x}'\\|_2^2)\n",
        "\\end{align} \n",
        "So this choice of random features approximate the desired RBF kernel with  $\\gamma=\\frac{1}{2\\sigma^2}$. "
      ],
      "metadata": {
        "id": "hXJ_vieYTyEy"
      }
    }
  ]
}