{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2styShouHgqeV4wwi6UKt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/AppliedStatistics/blob/main/Homework/HW4/565Sp23HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPY0Yzeq6dnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeab06d9-8929-438f-c3ed-25bc8e016148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline \n",
        "import pandas as pd\n",
        "from scipy import linalg\n",
        "from sklearn import tree\n",
        "from itertools import combinations\n",
        "import scipy\n",
        "import scipy.io as io\n",
        "from scipy.io import mmread\n",
        "import scipy.sparse as sparse\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mm#1{\\boldsymbol{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "$\\def\\mr#1{\\mathrm{#1}}$\n",
        "$\\newenvironment{rmat}{\\left[\\begin{array}{rrrrrrrrrrrrr}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\brm{\\begin{rmat}}$\n",
        "$\\newcommand\\erm{\\end{rmat}}$\n",
        "$\\newenvironment{cmat}{\\left[\\begin{array}{ccccccccc}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\bcm{\\begin{cmat}}$\n",
        "$\\newcommand\\ecm{\\end{cmat}}$\n",
        "# Homework 4\n",
        "## Homework guideline\n",
        "- The deadline is Apr 19th 10:30am. Submission after the deadline will not be graded. \n",
        "\n",
        "\n",
        "- Submit your work(your reasoning and your code) as a SINGLE .ipynb document. Please rename the document as _HW1_YOURNAME.ipynb_ (for example, _HW1_FELIX.ipynb_). You are responsible for checking that you have correctly submitted the correct document. If your code cannot run, you may receive NO point. \n",
        "\n",
        "- Please justify all short answers with a brief explanation. \n",
        "\n",
        "\n",
        "\n",
        "- You only use the Python packages included in the following cell. You are not allowed to use other advanced package or modules unless you are permitted to. \n",
        "\n",
        "- In your final submission include the plots produced by the unedited code as presented below, as well as any additional plots produced after editing the code during the course of a problem. You may find it necessary to copy/paste relevant code into additional cells to accomplish this.\n",
        "\n",
        "- Feel free to use the lecture notes and other resources. But you\n",
        "must understand, write, and hand in your own answers. In addition, you must write and submit\n",
        "your own code in the programming part of the assignment (we may run your code).\n",
        "If you copy someone else homework solution, both of you may receive ZERO point. \n",
        "\n",
        "\n",
        "- Colab is preferred. However, if you use Anaconda, please download the .mat file locally and save it to the same folder as this homework file. \n",
        "\n"
      ],
      "metadata": {
        "id": "QV2D7Og46iW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collaboration:** List the names of all people you collaborated with and for which question(s). This is important!"
      ],
      "metadata": {
        "id": "qwOQPe_F7eC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1 and Q2 together can be used to present in showcase day as extra credits.**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# Q1: SPECTF Heart Data (50pt)\n",
        "In this programming exercise, we will get started with using scikit-learn and python by using an existing\n",
        "decision tree implementation provided with scikit-learn.\n",
        "\n",
        "### Data Set Description\n",
        "We will be applying decision tree learning to the evaluation of cardiac Single Proton Emission Computed\n",
        "Tomography (SPECT) images. We will work with a database of 267 SPECT image sets, each of which corresponds to a patient. Each patient’s scan was classified as either “normal(0)” or “abnormal(1)” by a physician; your job is to train a **classifier** to automatically evaluate SPECT image sets based on this training data. Instead\n",
        "of working with raw image sets, each SPECT image set was processed to extract 44 continuous features\n",
        "that summarize the original SPECT images. Each feature is a number between 0 and 100 corresponding\n",
        "to a “region of interest” in the image during stress or at-rest tests. \n",
        "\n",
        "The data is given in `SPECTF.dat`: the\n",
        "first column represents the class label and the remaining columns represent the features. The SPECTF data\n",
        "originally came from http://archive.ics.uci.edu/ml/datasets/SPECTF+Heart.\n"
      ],
      "metadata": {
        "id": "5xearCYA2vLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStatistics/main/Homework/HW4/SPECTF.dat?raw=true -O SPECTF.dat"
      ],
      "metadata": {
        "id": "R3U2eLVi247f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96135829-3bf3-4de3-f5de-b9d9224f3fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-20 14:10:09--  https://raw.githubusercontent.com/yexf308/AppliedStatistics/main/Homework/HW4/SPECTF.dat?raw=true\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35764 (35K) [text/plain]\n",
            "Saving to: ‘SPECTF.dat’\n",
            "\n",
            "\rSPECTF.dat            0%[                    ]       0  --.-KB/s               \rSPECTF.dat          100%[===================>]  34.93K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-11-20 14:10:09 (37.7 MB/s) - ‘SPECTF.dat’ saved [35764/35764]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.loadtxt('SPECTF.dat', delimiter=',')\n",
        "X = data[:, 1:]\n",
        "y = np.array([data[:, 0]]).T\n",
        "n,d = X.shape"
      ],
      "metadata": {
        "id": "QAH0eqdD27Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "BlOo6VtK3NdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3eca2b9-aeca-4fa6-aa40-802ad6065552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(267, 44)\n",
            "(267, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Q1.1 Cross-validation (30pt)\n",
        "In this problem, we will 9-fold validate to determine the hyperparameter: the  maximum layer of the decision tree. \n",
        "1. Randomize the order of the instances in the data set, and split the data into training, validation and testing sets with 80%, 10% and 10%. \n",
        "\n",
        "2. Perform 9-fold cross validation over the training and validation datasets, and record **the training error and validation error** for the decision tree with maximum layer from 1 to 10. \n",
        "\n",
        "3. Since the contruction of decision tree in sklearn is random, please repeat the process of step 1 and 2 **100 trials**, and report **the mean and standard deviation** of the training error and validation error over all 100 trials of  9-fold validation. Be certain to shuffle the data at the start of each trial, but never within a trial.\n",
        "\n",
        "4. Plot mean of both errors in the same figure vs the maximum layer and report the optimal maximum layer. \n",
        "\n",
        "5. Combine the training and validation dataset, build the decision tree classifier with the optimal maximum layer and report the testing error on predicting the testing dataset.  \n",
        "\n",
        "Note although scikit-learn provides libraries that implement 9-fold validation, you may not use them\n",
        "for this assignment – you must implement 9-fold validation yourself.\n",
        "\n",
        "To display the standard deviations on the plot, see errorbar (http://matplotlib.org/examples/statistics/errorbar_demo_features.html) functions in matplotlib."
      ],
      "metadata": {
        "id": "e2S1gZeT3SbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1.1  your code starts here"
      ],
      "metadata": {
        "id": "eoy9yRQcRc34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "viviQ0ob3sbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Q1.2: Generating a Learning Curve (20pt) \n",
        "In this question, we will use the optimal maximum layer as we got from the last question. We combine the training and validation dataset into the total training set. \n",
        "\n",
        "We will generate and output a plot showing the learning curve over the total training data. The learning curve should **plot the mean and standard deviation of the testing error** for $10\\%, 20\\%, \\dots, 100\\% $ of the total training data. Note that $100\\%$ of the total training data corresponds to $90\\%$ of the complete data set. As before, the learning curve statistics should be computed over 100 trials for each different amount of training data. "
      ],
      "metadata": {
        "id": "Bcwg0O3b3zPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1.2  your code starts here"
      ],
      "metadata": {
        "id": "Ps_A9LZO3-hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "nFQwBNbv4MdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Important information\n",
        "One of the most difficult aspects of machine learning is that your classifier must generalize well to unseen\n",
        "data. In the following questions, we are supplying you with labeled training data and unlabeled test data. Specifically,\n",
        "**you will not have access to the labels for the test data**, which we will use to grade your homework. You will fit the best model that you can to the given data and then use that model to predict labels for the test data. It is these predicted labels that you will submit, and we will grade your submission based on your test\n",
        "accuracy (relative to the best performance you should be able to obtain). We will **compute your test accuracy based on your predicted labels for the test data and the true test labels**. Note also that we will not be providing any feedback on your predictions\n",
        "or your test accuracy when you submit your assignment, so you must do your best without feedback on your\n",
        "test performance."
      ],
      "metadata": {
        "id": "bFdid6YzIkWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Q2: Boosted Decision Tree (50pt)\n",
        "In class, we mentioned that boosted decision trees have been shown to be one of the best “out-of-the-box”\n",
        "classifiers. (That is, if you know nothing about the data set and can’t do parameter tuning, they will likely\n",
        "work quite well.) Boosting allows the decision trees to represent a much more complex decision surface than\n",
        "a single decision tree.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Q2.1 Implementation (30pt)\n",
        "Write a class that implements a boosted decision tree classifier. Your implementation may rely on the\n",
        "decision tree classifier already provided in `sklearn.tree.DecisionTreeClassifier`. But you\n",
        "**must implement the boosting process yourself**. (The scikit_learn module actually provides boosting as a\n",
        "meta-classifier, but you must not use it in your implementation.) Each decision tree in the ensemble should\n",
        "be limited to a maximum depth as specified in the `BoostedDT` constructor. You can configure the maximum\n",
        "depth of the tree via the `max_depth` argument to the `DecisionTreeClassifier` constructor.\n",
        "\n",
        "You must implement the following class: \n",
        "\n",
        "- `__init__(numBoostingIters = 100, maxTreeDepth = 3)`:  the constructor, which takes in the number of boosting iterations (default value: 100) and the maximum depth of the member decision trees (default: 3). \n",
        "\n",
        "- `fit(X,y)`: train the classifier from labeled data $(\\m{X}, \\m{y})$\n",
        "\n",
        "- `predict(X)`:  return an array of $N$ predictions for each of $N$ rows of $\\m{X}$. \n",
        "\n",
        "Be very careful not\n",
        "to change the class. You should configure your boosted decision tree classifier to be the best “out-of-the-box”\n",
        "classifier you can; you may not modify the constructor to take in additional parameters (e.g., to configure\n",
        "the individual decision trees).\n",
        "\n",
        "There is one additional change you need to make to **AdaBoost** beyond the algorithm described in class.\n",
        "AdaBoost by default only works with binary classes, but in this case, we have a multi-class classification\n",
        "problem. One variant of AdaBoost, called **AdaBoost-SAMME**, easily adapts AdaBoost to multiple classes.\n",
        "Instead of using the equation $\\beta_t = \\frac{1}{2}\\ln\\left(\\frac{1-\\epsilon}{\\epsilon}\\right)$ in AdaBoost, , you should use the AdaBoost-SAMME equation \n",
        "\\begin{align}\n",
        "\\beta_t = \\frac{1}{2}\\left(\\ln\\left(\\frac{1-\\epsilon}{\\epsilon}\\right)+\\ln(K-1)\\right)\n",
        "\\end{align}\n",
        "where $K$ is the total number of classes. This will force $\\beta_t\\ge 0$  as long as the classifier is no worse than\n",
        "random guessing. Note that when $K=2$, AdaBoost-SAMME reduces to AdaBoost. \n",
        "\n",
        "You should test your `BoostedDT` model to a\n",
        "regular decision tree on the iris data with a 50:50 training/testing split. You should see that your `BoostedDT`\n",
        "model is able to obtain 97% accuracy vs the 96% accuracy of regular decision trees. Make certain that your\n",
        "implementation works correctly before moving on to the next part."
      ],
      "metadata": {
        "id": "IzBhO8uB89Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2.1 \n",
        "class BoostedDT:\n",
        "\n",
        "    def __init__(self, num_boosting_iters=100, max_tree_depth=3):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains the model\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy array\n",
        "            y is an n-dimensional numpy array\n",
        "        \"\"\"\n",
        "        # TODO: np.unique and np logical functions (logical_and/or/not) may be helpful to your implementation\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Used the model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy array\n",
        "        Returns:\n",
        "            an n-dimensional numpy array of the predictions\n",
        "        \"\"\"\n",
        "        # TODO\n"
      ],
      "metadata": {
        "id": "LYvQiivRA5vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test your code \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris \n",
        "\n",
        "# load the data set\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "n, d = X.shape\n",
        "nTrain = int(0.5*n)  # training on 50% of the data\n",
        "\n",
        "# shuffle the data\n",
        "idx = np.arange(n)\n",
        "np.random.seed(13)\n",
        "np.random.shuffle(idx)\n",
        "X = X[idx]\n",
        "y = y[idx]\n",
        "\n",
        "# split the data\n",
        "Xtrain = X[:nTrain, :]\n",
        "ytrain = y[:nTrain]\n",
        "Xtest = X[nTrain:, :]\n",
        "ytest = y[nTrain:]\n",
        "\n",
        "# train the decision tree\n",
        "modelDT = DecisionTreeClassifier()\n",
        "modelDT.fit(Xtrain, ytrain)\n",
        "\n",
        "# train the boosted DT\n",
        "modelBoostedDT = BoostedDT(num_boosting_iters=100, max_tree_depth=2)\n",
        "modelBoostedDT.fit(Xtrain, ytrain)\n",
        "\n",
        "# output predictions on the remaining data\n",
        "ypred_DT = modelDT.predict(Xtest)\n",
        "ypred_BoostedDT = modelBoostedDT.predict(Xtest)\n",
        "\n",
        "# compute the training accuracy of the model\n",
        "accuracyDT = accuracy_score(ytest, ypred_DT)\n",
        "accuracyBoostedDT = accuracy_score(ytest, ypred_BoostedDT)\n",
        "\n",
        "print(\"Decision Tree Accuracy = \" + str(accuracyDT))\n",
        "print(\"Boosted Decision Tree Accuracy = \" + str(accuracyBoostedDT))"
      ],
      "metadata": {
        "id": "PzEidNxZCBBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "### Q2.2: Generalizing to Unseen Data (5pt)\n",
        "\n",
        "Once your boosted decision tree is working, train your `BoostedDT` on the labeled data available in the file: `challengeTrainLabeled.txt`.  The class labels are specified in the last column of data.  Then, use the trained `BoostedDT` classifier to predict a label $y\\in\\{1,\\dots, 9\\}$  for each unlabeled instance in `challengeTestUnlabeled.txt`. \n",
        "\n",
        "- Record the expected accuracy of your model. \n",
        "- Your implementation should output a comma-separated list of predicted labels, such as,\n",
        "1, 2, 1, 9, 4, 1, 3, 1, 5, 3, 4, 2, 8, 3, 1, 6, 3 ...\n",
        " \n",
        "- Save the comma-separated list into\n",
        "a text file named `predictions-BoostedDT.txt`. You may use the following command: `np.savetxt('predictions-BoostedDT.txt', predictions, delimiter=',')`. Your file will be saved in the folder of colab and you can download that from the file tab. \n",
        "\n",
        "\n",
        "\n",
        "Be very careful not to shuffle the instances in `challengeTestUnlabeled.txt`.  The first predicted label\n",
        "should correspond to the first unlabeled instance in the testing data. The number of predictions should\n",
        "match the number of unlabeled test instance.\n"
      ],
      "metadata": {
        "id": "E_RgYGcPkDi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStatistics/main/Homework/HW5/challengeTrainLabeled.txt?raw=true -O challengeTrainLabeled.txt\n",
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStatistics/main/Homework/HW5/challengeTestUnlabeled.txt?raw=true -O challengeTestUnlabeled.txt"
      ],
      "metadata": {
        "id": "R3M9d-75jhDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = np.loadtxt('challengeTrainLabeled.txt', delimiter=',')\n",
        "test  = np.loadtxt('challengeTestUnlabeled.txt', delimiter=',')"
      ],
      "metadata": {
        "id": "4RzKV-akjyNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2.2 Your code starts here. "
      ],
      "metadata": {
        "id": "laPehOMVqdh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Q2.3 Training the Best Classifier (15pt) \n",
        "Now, train the very best classifier for the challenge data, and use that classifier to output a second vector\n",
        "of predictions for the test instances. You may use any machine learning algorithm you like, and may tune\n",
        "it any way you wish. You may use the method and helper functions built into scikit_learn.  you do not need\n",
        "to implement the method yourself, but may if you wish. If you can think of a way that the unlabeled data in `challengeTestUnlabeled.txt` would be useful during the training process, you are welcome to let\n",
        "your classifier have access to it during training.\n",
        "\n",
        "Again, be careful not to shuffle the test instances; the\n",
        "order of the predictions must match the order of the test instances. \n",
        "\n",
        "- Record the expected accuracy of your model.\n",
        "\n",
        "- save the comma-separated list into a\n",
        "text file named `predictions-BestClassifier.txt`. this file should have one line of text that contains the\n",
        "list of predictions.\n",
        "\n",
        "- Write a brief paragraph (6–8 sentences max) describing the best machine learning classifier you found, its\n",
        "optimal parameter settings (if any), and how you trained the model."
      ],
      "metadata": {
        "id": "UzMBneFnrGaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2.3 Your code starts here. "
      ],
      "metadata": {
        "id": "zbRMulsgBuYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "JAN2xmi0Bw0N"
      }
    }
  ]
}