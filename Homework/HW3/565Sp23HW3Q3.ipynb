{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMW3gD1/6S+rT71gP7E2TG5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/AppliedStatistics/blob/main/Homework/HW3/565Sp23HW3Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lRxrr9CMy-N",
        "outputId": "aa6997db-22af-4e65-9c6a-a79553aa6d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline \n",
        "import pandas as pd\n",
        "from scipy import linalg\n",
        "from itertools import combinations\n",
        "import scipy\n",
        "import scipy.io as io\n",
        "import scipy.sparse as sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3: Polynomial Regression and Learning Curve\n",
        "Recall that polynomial regression learns a function $h_{\\mm{\\theta}}(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\ldots + \\theta_d x^d$.  In this case, $d$ represents the polynomial's degree.  We can equivalently write this in the form of a  linear model\n",
        "\\begin{equation}\n",
        "h_{\\mm{\\theta}}(x) = \\theta_0 \\phi_0(x) + \\theta_1 \\phi_1(x)  + \\theta_2 \\phi_2(x)  + \\ldots + \\theta_d \\phi_d(x)  \\enspace ,\n",
        "\\end{equation}\n",
        "using the basis expansion that $\\phi_j(x) = x^j$.  Notice that, with this basis expansion, we obtain a linear model where the features are various powers of the single univariate $x$.  We're still solving a linear regression problem, but are fitting a polynomial function of the input.\n",
        "\n",
        "## Q3.1: Implement regularized polynomial regression (20pt)\n",
        "You may implement it however you like, using a closed-form solution.  We've included an example closed-form implementation of linear regression (you are welcome to build upon this implementation, but make CERTAIN you understand it, since you'll need to change several lines of it).  You are also welcome to build upon your implementation from the previous assignment, but you must follow the API below.  Note that all matrices are actually 2D numpy arrays in the implementation.\n",
        "\n",
        "-  ```__init__(degree=1, regLambda=1E-8)``` : constructor with arguments of $d$ and $\\lambda$.\n",
        "\n",
        "- ```fit(X,Y)```: method to train the polynomial regression model\n",
        "\n",
        "- ```predict(X)```: method to use the trained polynomial regression model for prediction.\n",
        "\n",
        "-  ```polyfeatures(X, degree)```: expands the given $n \\times 1$ matrix $X$ into an $n \\times d$ matrix of polynomial features of degree $d$.  Note that the returned matrix will not include the zero-th power.\n",
        "\n",
        "\n",
        "Note that the ```polyfeatures(X, degree)``` function maps the original univariate data into its higher order powers.  Specifically, $X$ will be an $n \\times 1$ matrix $(X \\in \\mathbb{R}^{n \\times 1})$ and this function will return the polynomial expansion of this data, a $n \\times d$ matrix.  Note that this function will **not** add in the zero-th order feature (i.e., $x_0 = 1$).  You should add the $x_0$ feature separately, outside of this function, before training the model.\n",
        "\n",
        "By not including the $x_0$ column in the matrix ```polyfeatures()```, this allows the ```polyfeatures``` function to be more general, so it could be applied to multi-variate data as well. (If it did add the $x_0$ feature, we'd end up with multiple columns of 1's for multivariate data.)\n",
        "\n",
        "Also, notice that the resulting features will be badly scaled if we use them in raw form.  For example, with a polynomial of degree $d = 8$ and $x = 20$, the basis expansion yields $x^1 = 20$ while $x^8 = 2.56 \\times 10^{10}$ -- an\n",
        "absolutely huge difference in range.  Consequently, we will need to standardize the data before solving linear regression.  Standardize the data in ```fit()``` after you perform the polynomial feature expansion.  You'll need to apply the same standardization transformation in ```predict()``` before you apply it to new data.\n"
      ],
      "metadata": {
        "id": "yeBqhyLBy_Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "      Sample implementation of linear regression using direct computation of the solution\n",
        "\n",
        "\"\"\"\n",
        "#-----------------------------------------------------------------\n",
        "#  Class LinearRegression - Closed Form Implementation\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "class LinearRegressionClosedForm:\n",
        "\n",
        "    def __init__(self, reg_lambda=1E-8):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        self.regLambda = reg_lambda\n",
        "        self.theta = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "            Trains the model\n",
        "            Arguments:\n",
        "                X is a n-by-d array\n",
        "                y is an n-by-1 array\n",
        "            Returns:\n",
        "                No return value\n",
        "        \"\"\"\n",
        "        n = len(X)\n",
        "\n",
        "        # add 1s column\n",
        "        X_ = np.c_[np.ones([n, 1]), X]\n",
        "\n",
        "        n, d = X_.shape\n",
        "        d = d-1  # remove 1 for the extra column of ones we added to get the original num features\n",
        "\n",
        "        # construct reg matrix\n",
        "        reg_matrix = self.regLambda * np.eye(d + 1)\n",
        "        reg_matrix[0, 0] = 0\n",
        "\n",
        "        # analytical solution (X'X + regMatrix)^-1 X' y\n",
        "        self.theta = np.linalg.pinv(X_.T.dot(X_) + reg_matrix).dot(X_.T).dot(y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy array\n",
        "        Returns:\n",
        "            an n-by-1 numpy array of the predictions\n",
        "        \"\"\"\n",
        "        n = len(X)\n",
        "\n",
        "        # add 1s column\n",
        "        X_ = np.c_[np.ones([n, 1]), X]\n",
        "\n",
        "        # predict\n",
        "        return X_.dot(self.theta)\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "#  End of Class LinearRegression - Closed Form Implementation\n",
        "#-----------------------------------------------------------------"
      ],
      "metadata": {
        "id": "cnHGIVnF1WL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------------\n",
        "#  Class PolynomialRegression\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "class PolynomialRegression:\n",
        "\n",
        "    def __init__(self, degree=1, reg_lambda=1E-8):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        #TODO\n",
        "\n",
        "    def polyfeatures(self, X, degree):\n",
        "        \"\"\"\n",
        "        Expands the given X into an n * d array of polynomial features of\n",
        "            degree d.\n",
        "\n",
        "        Returns:\n",
        "            A n-by-d numpy array, with each row comprising of\n",
        "            X, X * X, X ** 3, ... up to the dth power of X.\n",
        "            Note that the returned matrix will not include the zero-th power.\n",
        "\n",
        "        Arguments:\n",
        "            X is an n-by-1 column numpy array\n",
        "            degree is a positive integer\n",
        "        \"\"\"\n",
        "        #TODO\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "            Trains the model\n",
        "            Arguments:\n",
        "                X is a n-by-1 array\n",
        "                y is an n-by-1 array\n",
        "            Returns:\n",
        "                No return value\n",
        "            Note:\n",
        "                You need to apply polynomial expansion and scaling\n",
        "                at first\n",
        "        \"\"\"\n",
        "        #TODO\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-1 numpy array\n",
        "        Returns:\n",
        "            an n-by-1 numpy array of the predictions\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "#  End of Class PolynomialRegression\n",
        "#-----------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "Gglnuu5p3uJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will test your implementation, which will plot the learned function.  In this case, the script fits a polynomial of degree $d=8$ with no regularization $\\lambda = 0$.  From the plot, we see that the function fits the data well, but will not generalize well to new data points.  Try increasing the amount of regularization, and examine the resulting effect on the function."
      ],
      "metadata": {
        "id": "E7fE8IbV3_iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStatistics/main/Homework/HW3/polydata.dat?raw=true -O polydata.dat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-1Bp2WJ4Nds",
        "outputId": "76262646-16c8-4410-f8ea-80eef40db820"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-12 19:49:26--  https://raw.githubusercontent.com/yexf308/AppliedStatistics/main/Homework/HW3/polydata.dat?raw=true\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57 [text/plain]\n",
            "Saving to: ‘polydata.dat’\n",
            "\n",
            "polydata.dat        100%[===================>]      57  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-12 19:49:26 (1.65 MB/s) - ‘polydata.dat’ saved [57/57]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allData = np.loadtxt('polydata.dat', delimiter=',')\n",
        "\n",
        "X = allData[:, [0]]\n",
        "y = allData[:, [1]]\n",
        "\n",
        "# regression with degree = d\n",
        "d = 8\n",
        "model = PolynomialRegression(degree=d, reg_lambda=0)\n",
        "model.fit(X, y)\n",
        "\n",
        "# output predictions\n",
        "xpoints = np.linspace(np.max(X), np.min(X), 100).reshape(-1, 1)\n",
        "ypoints = model.predict(xpoints)\n",
        "\n",
        "# plot curve\n",
        "plt.figure()\n",
        "plt.plot(X, y, 'rx')\n",
        "plt.title('PolyRegression with d = '+str(d))\n",
        "plt.plot(xpoints, ypoints, 'b-')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PYbCWJV74oix"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3.2: Learning Curve (20pt)\n",
        "\n",
        " In this problem we will examine the bias-variance tradeoff through learning curves. Learning curves provide a valuable mechanism for evaluating the bias-variance tradeoff. Implement the ```learningCurve()``` function to compute the learning curves for a given training/test set.  The ```learningCurve(Xtrain, ytrain, Xtest, ytest, degree, regLambda)``` function should take in the training data (```Xtrain```, ```ytrain```), the testing data (```Xtest```, ```ytest```), and values for the polynomial degree $d$ and regularization parameter $\\lambda$. \n",
        "\n",
        "The function should return two arrays, ```errorTrain``` (the array of training errors) and ```errorTest``` (the array of testing errors).  The $i^{th}$ index (start from 0) of each array should return the training error (or testing error) for learning with $i +1$ training instances.  Note that the 0$^{th}$ index actually won't matter, since we typically start displaying the learning curves with two or more instances.\n",
        "\n",
        "When computing the learning curves, you should learn on ```Xtrain```[0:$i$] for $i = 1, \\ldots, \\text{numInstances}(\\texttt{Xtrain})+1$, each time computing the testing error over the **entire** test set.  There is no need to shuffle the training data, or to average the error over multiple trials -- just produce the learning curves for the given training/testing sets with the instances in their given order.  Recall that the error for regression problems is given by\n",
        "\\begin{equation}\n",
        "\\frac{1}{n} \\sum_{i=1}^n (h_{\\mm{\\theta}}(\\mathbf{x}_i) - y_i)^2 \\enspace .\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "0xrHCVWY5wLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learningCurve(Xtrain, Ytrain, Xtest, Ytest, reg_lambda, degree):\n",
        "    \"\"\"\n",
        "    Compute learning curve\n",
        "\n",
        "    Arguments:\n",
        "        Xtrain -- Training X, n-by-1 matrix\n",
        "        Ytrain -- Training y, n-by-1 matrix\n",
        "        Xtest -- Testing X, m-by-1 matrix\n",
        "        Ytest -- Testing Y, m-by-1 matrix\n",
        "        regLambda -- regularization factor\n",
        "        degree -- polynomial degree\n",
        "\n",
        "    Returns:\n",
        "        errorTrain -- errorTrain[i] is the training accuracy using\n",
        "        model trained by Xtrain[0:(i+1)]\n",
        "        errorTest -- errorTrain[i] is the testing accuracy using\n",
        "        model trained by Xtrain[0:(i+1)]\n",
        "\n",
        "    Note:\n",
        "        errorTrain[0:1] and errorTest[0:1] won't actually matter, since we start displaying the learning curve at n = 2 (or higher)\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(Xtrain)\n",
        "\n",
        "    errorTrain = np.zeros(n)\n",
        "    errorTest = np.zeros(n)\n",
        "\n",
        "    #TODO -- complete rest of method; errorTrain and errorTest are already the correct shape\n",
        "\n",
        "    return errorTrain, errorTest\n"
      ],
      "metadata": {
        "id": "gigFqFas7nm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the function is written to compute the learning curves, you should test with the following code to plot the learning curves for various values of $\\lambda$ and $d$. \n",
        "\n",
        "You should see plots similar to the following\n",
        "\n",
        "<img src=\"https://github.com/yexf308/AppliedStatistics/blob/main/image/learning.png?raw=true\" width=\"700\" />\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D6UBCwcG8JPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------------\n",
        "# Plotting tools\n",
        "\n",
        "def plotLearningCurve(errorTrain, errorTest, regLambda, degree):\n",
        "    \"\"\"\n",
        "        plot computed learning curve\n",
        "    \"\"\"\n",
        "    minX = 3\n",
        "    maxY = max(errorTest[minX+1:])\n",
        "\n",
        "    xs = np.arange(len(errorTrain))\n",
        "    plt.plot(xs, errorTrain, 'r-o')\n",
        "    plt.plot(xs, errorTest, 'b-o')\n",
        "    plt.plot(xs, np.ones(len(xs)), 'k--')\n",
        "    plt.legend(['Training Error', 'Testing Error'], loc='best')\n",
        "    plt.title('Learning Curve (d='+str(degree)+', lambda='+str(regLambda)+')')\n",
        "    plt.xlabel('Training samples')\n",
        "    plt.ylabel('Error')\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(top=maxY)\n",
        "    plt.xlim((minX, 10))\n",
        "\n",
        "\n",
        "def generateLearningCurve(X, y, degree, regLambda):\n",
        "    \"\"\"\n",
        "        computing learning curve via leave one out CV\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(X)\n",
        "\n",
        "    errorTrains = np.zeros((n, n-1))\n",
        "    errorTests = np.zeros((n, n-1))\n",
        "\n",
        "    loo = model_selection.LeaveOneOut()\n",
        "    itrial = 0\n",
        "    for train_index, test_index in loo.split(X):\n",
        "        #print(\"TRAIN indices:\", train_index, \"TEST indices:\", test_index)\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        (errTrain, errTest) = learningCurve(X_train, y_train, X_test, y_test, regLambda, degree)\n",
        "\n",
        "        errorTrains[itrial, :] = errTrain\n",
        "        errorTests[itrial, :] = errTest\n",
        "        itrial = itrial + 1\n",
        "\n",
        "    errorTrain = errorTrains.mean(axis=0)\n",
        "    errorTest = errorTests.mean(axis=0)\n",
        "\n",
        "    plotLearningCurve(errorTrain, errorTest, regLambda, degree)\n",
        "\n"
      ],
      "metadata": {
        "id": "BhwNx0lJ8d-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allData = np.loadtxt('polydata.dat', delimiter=',')\n",
        "\n",
        "X = allData[:, [0]]\n",
        "y = allData[:, [1]]\n",
        "\n",
        "# generate Learning curves for different params\n",
        "plt.figure(figsize=(15, 9), dpi=100)\n",
        "plt.subplot(2, 3, 1)\n",
        "generateLearningCurve(X, y, 1, 0)\n",
        "plt.subplot(2, 3, 2)\n",
        "generateLearningCurve(X, y, 4, 0)\n",
        "plt.subplot(2, 3, 3)\n",
        "generateLearningCurve(X, y, 8, 0)\n",
        "plt.subplot(2, 3, 4)\n",
        "generateLearningCurve(X, y, 8, .1)\n",
        "plt.subplot(2, 3, 5)\n",
        "generateLearningCurve(X, y, 8, 1)\n",
        "plt.subplot(2, 3, 6)\n",
        "generateLearningCurve(X, y, 8, 100)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dVHa19EJ9JAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the following:\n",
        "\n",
        "- The y-axis is using a log-scale and the ranges of the y-scale are all different for the plots.  The dashed black line indicates the $y=1$ line as a point of reference between the plots.\n",
        "- The plot of the unregularized model with $d = 1$ shows poor training error, indicating a high bias (i.e., it is a standard univariate linear regression fit).\n",
        "- The plot of the unregularized model ($\\lambda = 0$) with $d = 8$ shows that the training error is low, but that the testing error is high.  There is a huge gap between the training and testing errors caused by the model overfitting the training data, indicating a high variance problem.\n",
        "- As the regularization parameter increases (e.g., $\\lambda = 1$) with $d = 8$, we see that the gap between the training and testing error narrows, with both the training and testing errors converging to a low value.  We can see that the model fits the data well and generalizes well, and therefore does not have either a high bias or a high variance problem.  Effectively, it has a good tradeoff between bias and variance.\n",
        "- Once the regularization parameter is too high ($\\lambda = 100$), we see that the training and testing errors are once again high, indicating a poor fit.  Effectively, there is too much regularization, resulting in high bias.\n",
        "\n",
        "\n",
        "Make absolutely certain that you understand these observations, and how they relate to the learning curve plots.  In practice, we can choose the value for $\\lambda$ via cross-validation to achieve the best bias-variance tradeoff."
      ],
      "metadata": {
        "id": "rK_v8NXQ8dKC"
      }
    }
  ]
}